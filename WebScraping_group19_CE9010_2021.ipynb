{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Acquisition through Web Scraping\n",
    "<hr> The objective is to scrape data from a website, namely SgCarMart.com , and to complile the data scraped into a csv file using BeautifulSoup. The main goal is to predict the price of a used car. through its price, depreciation per year, date of purchase. Hence we chose to use their advanced statistics available from their college play as the dependent variable to predict for their first two years in the league. Data extract data are from a website, sgcarmarts.com, and consist of data from the used car category as that is the targeted group. This source is chosen because of its good reputation in the second hand automotive industry. </hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import re\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###   Write headings in csv file for the relevant columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_csv_header():\n",
    "    fields = ['Img_url','Make & Model','Price','Depreciation','Reg Date','Eng Cap','Mileage','Vechical Type','Status']\n",
    "    \n",
    "    with open('results.csv', 'w') as f:\n",
    "        write = csv.writer(f,quoting=csv.QUOTE_ALL) \n",
    "        write.writerow(fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Data to the csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_data_to_csv(n):\n",
    "    with open('results.csv', 'a',newline='') as f:\n",
    "        write = csv.writer(f,quoting=csv.QUOTE_ALL)\n",
    "        write.writerow(n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To scrape data from url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, headers):\n",
    "    #Putting scrapped data into lists\n",
    "    Images_link = []\n",
    "    Make_Model  = []\n",
    "    Price       = []\n",
    "    Depreciation= []\n",
    "    Date        = []\n",
    "    Eng_cap     = []\n",
    "    Mileage     = []\n",
    "    Veh_type    = []\n",
    "    Status      = []\n",
    "    \n",
    " \n",
    "    req = requests.get(url, headers)\n",
    "    soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "\n",
    "    #Scrape Image\n",
    "    img_link = soup.findAll('img')\n",
    "    for i in img_link :\n",
    "        title = i.get('title')\n",
    "        if(title == \"more details\"):\n",
    "            Images_link.append(str(i['src']))\n",
    "\n",
    "    #Scrape Make_Model\n",
    "    for j in soup.find_all('strong'):\n",
    "        for k in j.find_all('a'):\n",
    "            if(\"info.php?\" in k['href']):\n",
    "                Make_Model.append(k.text)\n",
    "\n",
    "    #Scrape Price\n",
    "    for l in soup.find_all(\"td\", class_=\"font_red\"):\n",
    "        Price.append((l.text).strip())\n",
    "\n",
    "    #Scrape Depreciation\n",
    "    for m in soup.find_all(\"td\", width=\"101\"):\n",
    "        if(\"/\" in m.text or \"N.A\"in m.text):\n",
    "            Depreciation.append((m.text).strip())\n",
    "\n",
    "    #Scrape Date \n",
    "    for n in soup.find_all(\"div\", style=\"width:89px;\"):\n",
    "        if(\"-\" in n.text or \"N.A.\" in n.text):\n",
    "            Date.append(n.text.strip())      \n",
    "        \n",
    "    #Scrape Cap\n",
    "    for o in soup.find_all(\"div\", style=\"width:84px;\"):\n",
    "            Eng_cap.append(o.text.strip())  \n",
    "\n",
    "    #Scrape Mileage\n",
    "    for p in soup.find_all(\"div\", style=\"width:83px;\"):\n",
    "            Mileage.append(p.text.strip())              \n",
    "\n",
    "    #Scrape Veh Type\n",
    "    for q in soup.find_all(\"div\", style=\"width:89px;\"):\n",
    "        for r in q.find_all('a'):\n",
    "            Veh_type.append(r.text)\n",
    "\n",
    "    #Scrape Status\n",
    "    for s in soup.find_all('strong'):\n",
    "        for t in s.find_all('font'):\n",
    "                Status.append(t.text)\n",
    "                \n",
    "    #Scraped Data to CSV\n",
    "    data_to_csv=[]\n",
    "    for a,b,c,d,e,f,g,h,i in zip(Images_link,\n",
    "                                 Make_Model,\n",
    "                                 Price,\n",
    "                                 Depreciation,\n",
    "                                 Date,\n",
    "                                 Eng_cap,\n",
    "                                 Mileage,\n",
    "                                 Veh_type,\n",
    "                                 Status):\n",
    "       lot = (a,b,c,d,e,f,g,h,i)\n",
    "       write_data_to_csv(list(lot))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing User agent \n",
    "<hr> Websites may use anti scraping tools to block python requests, thus changing user agent to a valid broswer may help in bypassing this issue. \n",
    "For the chosen website, sgcartmart.com, there is no issue without using user agent. I have included it in for future proofing in the event the website decides to block python requests. </hr>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\", \n",
    "    \"Accept-Encoding\": \"gzip, deflate\", \n",
    "    \"Accept-Language\": \"en-GB,en-US;q=0.9,en;q=0.8\", \n",
    "    \"Dnt\": \"1\", \n",
    "    \"Host\": \"httpbin.org\", \n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"User-Agent\":\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.97 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv_header()\n",
    "\n",
    "for x in range (0,30800,100):\n",
    "    get_data(f\"https://www.sgcarmart.com/used_cars/listing.php?BRSR={x}&RPG=100&VEH=0\", headers)      \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
